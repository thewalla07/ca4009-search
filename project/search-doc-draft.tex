\documentclass[a4paper,12pt]{article}
\usepackage{datetime}
\usepackage{geometry}

\geometry{a4paper}

\usepackage{graphicx}
\usepackage{svg}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{endnotes}
\let\footnote=\endnote
\setlength\parindent{0pt}
\setlength{\parskip}{6pt}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Search Technologies Report on Screencast Search\\CASE4 CA4009}
\author{Michael Wall 13522003 michael.wall22@mail.dcu.ie\\Jordan Healy 13379226 jordan.healy36@mail.dcu.ie}
\date{\today}

\begin{document}
\maketitle
\newpage

\tableofcontents

\newpage

\section{Disclaimer}
A report submitted to Dublin City University, School of Computing for module CA4009: Search Technologies, 2016/2017.

I understand that the University regards breaches of academic integrity and plagiarism as grave and serious.

I have read and understood the DCU Academic Integrity and Plagiarism Policy. I accept the penalties that may be imposed should I engage in practice or practices that breach this policy.

I have identified and included the source of all facts, ideas, opinions, viewpoints of others in the assignment references. Direct quotations, paraphrasing, discussion of ideas from books, journal articles, internet sources, module text, or any other source whatsoever are acknowledged and the sources cited are identified in the assignment references.

I declare that this material, which I now submit for assessment, is entirely my own work and has not been taken from the work of others save and to the extent that such work has been cited and acknowledged within the text of my work.

By signing this form or by submitting this material online I confirm that this assignment, or any part of it, has not been previously submitted by me or any other person for assessment on this or any other course of study. By signing this form or by submitting material for assessment online I confirm that I have read and understood DCU Academic Integrity and Plagiarism Policy (available at: http://www.dcu.ie/registry/examinations/index.shtml)

Name(s): \\Michael Wall\\Jordan Healy\\

Date:

\newpage

\section{Abstract}
This report details a specification for a search system for the CA4009 module.

The system will allow the user to search the contents of the lecture slides used by the lecturer and also to search the voiceover presented by the lecturer for extra information that may not have been included in the slides.

The plan for this system is to use image processing to recognise text boxes or lecture slides which contain strings of text from keyframes in the video.

Audio recognition will be used to process the the lecturer's speech. From this we will alow the user to search based on a text query.

Within our proposed system we have weighed up the benefits and limitations of each subsystem, and compared possible algorithms. We developed ideas for implementation and evaluation plans, along with a mockup of the proposed UI.

\section{Introduction}
The idea for this search system is to enable users to search presentation materials, specifically videos, with greater ease. At present, if a user wants to look for information on a topic, they may come across a video presentation on the topic. The video may be too long to watch all of if the user is only looking for a specific piece of information. The system aims to allow a user to search these styles of video to get straight to the section which is relevant to their information need.

The need for this system comes from researching college material using external sources. Sometimes these come in the form of lectures and screencasts from other colleges. These lectures may have the information that you are looking for but are too long to regard as a usable resource.

These lectures are usually in video format with presentation slides as the video and audio from the lecturer overlaid on the video. The system will allow the user to search the contents of the lecture slides used by the lecturer and also to search the voiceover presented by the lecturer for extra information that may not have been included in the slides.

The plan for this system is to use image processing to recognise text boxes or lecture slides which contain strings of text from keyframes in the video. The system will then use Optical Character Recognition (OCR) to convert the images into searchable text. In addition, the audio will be parsed using Automatic Speech Recognition (ASR) to retrieve searchable text. The user’s search request will then be applied to the output of the above functions to find sections in the video which may be relevant. These sections will be highlighted to the user so that they can jump to these points in the video.

\section{Functional Description}
Our system will use a combination of Optical Character Recognition and Automatic Speech Recognition to process a selected video. These are detailed further below.

Please see Figure \ref{fig:system-diagram} in appendix for the system diagram and Figure \ref{fig:ui-diagram} for the proposed UI.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{overall-system.eps}
\caption{Overall system}
\label{fig:system-diagram}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{all-ui.eps}
\caption{User interface for a user viewing a document}
\label{fig:ui-diagram}
\end{figure}

\section{Optical Character Recognition (OCR)}
For OCR we must split the video up into a series of images. These images will be frames of the video taken at regular intervals. If two or more images from these frames are a said to be 90\% similar then we assume we have a duplicate slide from the presentation video. For this, we remove these duplicates. Please see Figure \ref{fig:ocr-proc} in appendix.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{ocr-result.eps}
\caption{Summarisation of the entire OCR process}
\label{fig:ocr-proc}
\end{figure}

\subsection{OCR - Preprocessing}
These preprocessing steps are touched on briefly below; their full explanations can be found at the relevant citations.

Text localisation will be used to identify the position of potential text regions from an image.
\subsubsection{Canny filters}
\begin{description}

\item[Purpose] Identify text blocks, where the blocks are defined as rectangular regions containing one or more lines of text.\cite{vocr_recall}
\item[Advantages] Using probability for finding error rate.
\\ Localization and response.\cite{vocr_gurmukhi}
\\ Improving signal to noise ratio.\cite{vocr_gurmukhi}
\\ Better detection especially in noise conditions.\cite{vocr_gurmukhi}
\item[Disadvantages] Complex Computations.
\\ False zero crossing problem.
\\ Time consuming.\cite{vocr_gurmukhi}
\end{description}

\subsubsection{Baseline extraction}
\begin{description}
\item[Purpose] Isolate and extract these potential text regions.
\item[Advantages] Easy to use.
\\ It works well with the printed text.\cite{vocr_base}
\item[Disadvantages] Difficulty with handwriting text especially with handwritten words.\cite{vocr_base}
\end{description}


Text verification will be used to classify text lines into actual text regions.
\subsubsection{Normalisation}
\begin{description}
\item[Purpose] Create all text regions equal in size [1].
\item[Advantages] Reduces the number of false alarms and creates a more precise location for each text region [1].
\item[Disadvantages] If normalisation processes is not precise, output could be worse than input.
\end{description}

\subsubsection{Align images, remove noise and binarize them}
\begin{description}
\item[Purpose] So that the most effective OCR can be carried out [1].
\item[Advantages] Will generate better results for OCR.
\item[Disadvantages] If removing too much noise may remove vital pixels needed for OCR.
\end{description}

\subsection{OCR - Algorithms}
OCR algorithms fall into two categories, matrix matching or feature extraction (aka Intelligent character recognition).

Matrix matching works by comparing what the OCR sees as a character with a library of character matrices. When an image matches one of these prescribed matrices of dots, the computer labels that image as the corresponding ASCII value.

Feature extraction does not use strict matching to its prescribed templates (like matrix matching does). The computer looks for general features like open areas, closed shapes, diagonal lines and line intersections. This process is used more for written text and requires machine learning to implement.

\subsubsection{Matrix matching}
\begin{description}
\item[Advantages] Easier and more simple to implement.
\item[Disadvantages] Sensitive to different styles of text.\cite{vocr_grad}
\\ Sensitive to noise.\cite{vocr_grad}
\\ Sensitive to rotated characters.\cite{vocr_grad}
\end{description}

\subsubsection{Feature extraction}
\begin{description}
\item[Advantages] More versatile.
\item[Disadvantages] Harder to implement than matrix matching.
\end{description}

We decided upon matrix matching for our OCR algorithm. This is because feature extraction is used mainly for handwritten text, which doesn’t really apply to our use case since we can assume presentation slides will be in the form of typed text. We also feel that we can overcome the disadvantages of matrix matching within pre-processing and our implementation of the algorithm.

Now that our image has been pre-processed we can pass it to our OCR. Our matrix matching algorithm will work as follows and is an expansion on the procedure described by Junaid Tariq, Umar Nauman and Muhammad Umair Naru\cite{vocr_base}:

\begin{itemize}
\item
Save a database of all valid characters. Each entry will consist of the character name, height (the difference between the first and last black pixel encounter from top to bottom), width (the difference between the leftmost and rightmost black pixel), checksum (total number of pixels of that character) and font style. E.g. “B”, 14, 8, 71, “Ariel”

\item
We extract these details for each char from the binarized image, and query the database. We use hard matching to find the the exact values in the database, or soft matching if no results are returned.

\item
To address the issues of typical matrix matching, we will add the same characters different font styles to our database. This should help the OCR in finding different text styles.
\end{itemize}

\section{Automatic Speech Recognition (ASR)}

With current technology in automatic speech recognition it is not possible to generate a perfect transcription of audio. This is due to a combination of issues related to acoustics, language models and phonetics. The generated index data can be searched through, however it will have errors which will cause some search terms not to be found, even in a relevant piece of audio dialogue. This will in turn reduce the Mean Average Precision of the overall system. However, this might be tolerable if not all of the data is required to be relevant in a search, as long as we can have some keywords that are important identified. For example, if the lecturer speaks about a topic and mumbles through some of the sentences, but mentions one of our keywords. If this is recognised then it may be irrelevant whether or not the rest of the sentence was heard correctly. In our case, the direct translation of the audio is not necessarily what we are trying to obtain either, just a pointer to a section that might be relevant. The user can then navigate to this section and determine if it is of relevance for themselves.

For our videos, we will be processing them directly through YouTube, as this is where the majority of our target data is located. YouTube provides a developer application programming interface to access a caption track\cite{asr_cap} on a given video. This API allows the captions to be retrieved using a HTTP GET request.

In the best case scenario, the video will have been transcribed by the video uploader, or contributed by a viewer. This will provide a very reliable audio transcription to search through as phonetic stumbles such as \textit{um}, \textit{ah} and so on may be removed through stop word removal.

In cases where captions have not been manually provided, captions are generated using Google's speech-to-text engine. This engine works for over 80 languages.\cite{asr_speech} In my experience, this API provides a reasonably reliable level of accuracy, even in noisy or imperfect audio environments.

\subsection{Pros and cons}
Trusting Google's Machine Learning has it's advantages and disadvantages. For one, they are using Machine Learning to constantly improve their accuracy in translation, and they also have access to possibly the largest set of training data for such a task. This is also a built in solution to the videos, and does not require extra computational resources for the user to process a video. This preprocessing is done by YouTube when a video is uploaded.

On the other hand, the accuracy of this method is not as accurate as if we were to have videos manually transcribed. This would vastly improve our search capability and accuracy, but at the expense of costly man hours. It would also limit the number of videos our search application could be used for.

\subsection{Assumptions}
We are making assumptions about the data which a user want's to search. Such assumptions are that the videos the user wants exist on YouTube and not on a college website. It also assumes that the API provided by YouTube will be available indefinitely in the future.

\section{Implementation}
Before we can use the data resulting from the OCR and the ASR there is pre-processing to be done with regards to text retrieval. The entire system process will work as follows:

\subsection{Text retrieval}
Once we have performed the OCR on the video and ASR on the audio, we will be able to perform a search of our user's query on the text transcripts. We will take the following steps during our text retrieval process:

\begin{itemize}
\item
We will apply proter stemming\cite{porter_stem} to the target text sections and to the user query. This will remove suffixes from the text to improve the reliability of the system.

\item
Next we will remove stop words from the processed text. This will involve the usual stop words such as \textit{to}, \textit{and}, \textit{it}; in addition to this we will be looking to remove some context specific stop words such as \textit{uhh}, \textit{umm} and other delaying noises in speech.

\item
We will then use one of either TF-IDF\cite{tf_idf} or Okapi BM25\cite{bm_25} to search for relevant sections of visual or speech text.
\end{itemize}

\subsection{Vector space model}
Vector space model is a type of best match search that will return the documents to a user in order of relevance based on their query. In summary, it works by representing documents and queries on a plane and the relevance value is calculated by the cosine of the angle between these vectors.\cite{tf_idf}

Please see Figure \ref{fig:vector} in appendices.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{vector-space-model.eps}
\caption{\textit{Sim} is the function which calculates the similarity between documents and queries where \textit{q} is the given query and \textit{d(j)} is each document.\cite{tf_idf}}
\label{fig:vector}
\end{figure}


\subsection{Architecture diagram}
Please see Figure \ref{fig:arch-diagram} for the workflow diagram of the system.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{workflow-diagram.eps}
\caption{Workflow of the system architecture}
\label{fig:arch-diagram}
\end{figure}

\subsection{Test effectiveness of the system}
The two parameters we will need to evaluate for the success of our system will be effectiveness and efficiency.

We will use videos which we know the content of and search for sections which exist in the videos. We will evaluate the effectiveness based on the number of relevant sections found divided by the number of sections which were returned as relevant. This will give a percentage of precision for the system.

We will also evaluate the system using the A/B testing along with user feedback to find a level of relevancy which is apporopriate for highlighting sections in the system.

For efficiency we will look at time consumption of the system. We will aim to minimize our search time to be at least 60\% or less of the time a user would normally spend manually clicking through a video for content. We hope to achieve this by serving up our search results as soon as we get them. We can deliver the results of the audio processing quicer than the video processing in some cases because it will be pre-processed for the video.

\newpage

\begin{thebibliography}{9}

\bibitem{vocr_recall}
    D. Das,
    D. Chen
    and A.G. Hauptmann,
    ``Improving Multimedia Retrieval with a Video OCR'',
    in \emph{Dipanjan Das}.
    [Online]
    Available: http://www.dipanjandas.com/files/VOCR\_draft.pdf.
    Accessed: Nov, 23, 2016.

\bibitem{vocr_gurmukhi}
    G. S. Lehal
    and An Singh,
    ``Feature Extraction and Classification for OCR of Gurmukhi Script'',
    in \emph{CiteSeer\textsuperscript{x}}.
    [Online]
    Available: http://www.learnpunjabi.org/pdf/vivek1.pdf.
    Accessed: Nov, 23, 2016.

\bibitem{vocr_base}
    J. Tariq,
    U. Nauman
    and M. Umair Naru,
    ``α-Soft: An English language OCR'',
    \emph{2010 2nd International Conference on Computer Engineering and Technology},
    Chengdu,
    2010,
    in \emph{IEEEXplore\textregistered}.
    [Online]
    Available: http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5486152.
    Accessed: Nov, 23, 2016.

\bibitem{vocr_grad}
    Chen Yu,
    Chen Dian-ren,
    Li Yang
    and Chen Lei,
    ``Otsu's thresholding method based on gray level-gradient two-dimensional histogram'',
    \emph{2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)},
    Wuhan,
    2010,
    in \emph{IEEEXplore\textregistered}.
    [Online]
    Available: http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456687.
    Accessed: Nov, 23, 2016.

\bibitem{asr_cap}
    ``YouTube Data API, Captions: download'',
    2016,
    in \emph{Google Developers}.
    [Online]
    Available: https://developers.google.com/youtube/v3/docs/captions/download.
    Accessed: Dec, 2, 2016.

\bibitem{asr_speech}
    ``Cloud Speech API\textsuperscript{beta}'',
    2016,
    in \emph{Google Cloud Platform}.
    [Online]
    Available: https://cloud.google.com/speech/.
    Accessed: Dec, 2, 2016.

\bibitem{porter_stem}
    C. J. van Rijsbergen,
    S. E. Robertson
    and M. F. Porter,
    ``New models in probabilistic information retrieval'',
    1980,
    London,
    British Library Research and Development Report, no. 5587.

\bibitem{tf_idf}
    G. Jones,
    ``Text Retrieval'',
    2016,
    in \emph{Loop}.
    [Online]
    Available: https://loop.dcu.ie/pluginfile.php/755925/mod\_resource/content/5/ir.pdf.
    Accessed: Nov, 23, 2016.

\bibitem{bm_25}
    G. Jones,
    ``Text Retrieval - BM25'',
    2016,
    in \emph{Loop}.
    [Online]
    Available: https://loop.dcu.ie/pluginfile.php/821757/mod\_resource/content/2/ir\_BM25.pdf.
    Accessed: Nov, 23, 2016.



\end{thebibliography}

\end{document}
